# arXivã‹ã‚‰é‡å­å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’å…¥æ‰‹ã™ã‚‹æ–¹æ³•

## ğŸ“š arXivãƒ‡ãƒ¼ã‚¿å–å¾—æˆ¦ç•¥

### 1. fetch_arxiv_quantum_data.py ã‚’ä½œæˆ

```python
"""
arXivã‹ã‚‰é‡å­è¨ˆç®—ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è«–æ–‡ã‚’æ¤œç´¢ãƒ»å–å¾—
"""
import requests
import xml.etree.ElementTree as ET
import pandas as pd
import re
import os
from datetime import datetime
import time

class ArxivQuantumDataFetcher:
    """arXivã‹ã‚‰é‡å­å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, save_dir='arxiv_quantum_data'):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        self.base_url = 'http://export.arxiv.org/api/query'
        
    def search_quantum_datasets(self, max_results=100):
        """é‡å­è¨ˆç®—ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è«–æ–‡ã‚’æ¤œç´¢"""
        
        # æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆè¤‡æ•°ã®æ¡ä»¶ã‚’ORæ¤œç´¢ï¼‰
        search_queries = [
            'all:"quantum computing" AND all:"experimental data"',
            'all:"quantum" AND all:"dataset" AND all:"measurement"',
            'all:"bell state" AND all:"experimental results"',
            'all:"quantum supremacy" AND all:"data"',
            'all:"NISQ" AND all:"benchmarks"',
            'all:"quantum processor" AND all:"calibration data"'
        ]
        
        all_papers = []
        
        for query in search_queries:
            print(f"\næ¤œç´¢ä¸­: {query}")
            
            params = {
                'search_query': query,
                'start': 0,
                'max_results': max_results // len(search_queries),
                'sortBy': 'submittedDate',
                'sortOrder': 'descending'
            }
            
            response = requests.get(self.base_url, params=params)
            time.sleep(3)  # arXivã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            
            if response.status_code == 200:
                papers = self._parse_arxiv_response(response.text)
                all_papers.extend(papers)
                print(f"  â†’ {len(papers)}ä»¶ã®è«–æ–‡ã‚’ç™ºè¦‹")
            
        # é‡è¤‡ã‚’é™¤å»
        unique_papers = {p['arxiv_id']: p for p in all_papers}.values()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›
        df = pd.DataFrame(list(unique_papers))
        df.to_csv(os.path.join(self.save_dir, 'arxiv_papers_with_data.csv'), index=False)
        
        return df
    
    def _parse_arxiv_response(self, xml_content):
        """arXiv APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹"""
        root = ET.fromstring(xml_content)
        
        # åå‰ç©ºé–“ã®å®šç¾©
        namespaces = {
            'atom': 'http://www.w3.org/2005/Atom',
            'arxiv': 'http://arxiv.org/schemas/atom'
        }
        
        papers = []
        
        for entry in root.findall('atom:entry', namespaces):
            # åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
            paper = {
                'title': entry.find('atom:title', namespaces).text.strip(),
                'arxiv_id': entry.find('atom:id', namespaces).text.split('/')[-1],
                'published': entry.find('atom:published', namespaces).text,
                'summary': entry.find('atom:summary', namespaces).text.strip(),
                'authors': [author.find('atom:name', namespaces).text 
                           for author in entry.findall('atom:author', namespaces)],
                'pdf_link': None,
                'has_data': False,
                'data_keywords': []
            }
            
            # PDFãƒªãƒ³ã‚¯ã‚’æ¢ã™
            for link in entry.findall('atom:link', namespaces):
                if link.get('type') == 'application/pdf':
                    paper['pdf_link'] = link.get('href')
                    break
            
            # ãƒ‡ãƒ¼ã‚¿ã®å­˜åœ¨ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
            abstract_lower = paper['summary'].lower()
            data_indicators = [
                'dataset', 'data set', 'experimental data', 'measurement data',
                'raw data', 'supplementary data', 'data available',
                'github.com', 'zenodo.org', 'figshare.com', 'dryad',
                'supplemental material', 'ancillary files'
            ]
            
            found_keywords = []
            for indicator in data_indicators:
                if indicator in abstract_lower:
                    found_keywords.append(indicator)
                    paper['has_data'] = True
            
            paper['data_keywords'] = found_keywords
            
            # GitHubãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            github_pattern = r'github\.com/[\w-]+/[\w-]+'
            github_matches = re.findall(github_pattern, abstract_lower)
            if github_matches:
                paper['github_links'] = github_matches
            
            papers.append(paper)
        
        return papers
    
    def get_supplementary_data_links(self, arxiv_id):
        """è«–æ–‡ã®è£œè¶³ãƒ‡ãƒ¼ã‚¿ã¸ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—"""
        
        # arXivã®ancillary filesã‚’ãƒã‚§ãƒƒã‚¯
        ancillary_url = f'https://arxiv.org/src/{arxiv_id}'
        
        # è«–æ–‡ãƒšãƒ¼ã‚¸ã‹ã‚‰è¿½åŠ æƒ…å ±ã‚’å–å¾—
        paper_url = f'https://arxiv.org/abs/{arxiv_id}'
        
        try:
            response = requests.get(paper_url)
            if response.status_code == 200:
                # HTMLã‹ã‚‰è¿½åŠ ã®ãƒ‡ãƒ¼ã‚¿ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                content = response.text
                
                # ã‚ˆãã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒªãƒã‚¸ãƒˆãƒªã®ãƒ‘ã‚¿ãƒ¼ãƒ³
                patterns = {
                    'github': r'https?://github\.com/[\w-]+/[\w-]+',
                    'zenodo': r'https?://zenodo\.org/record/\d+',
                    'figshare': r'https?://figshare\.com/[\w/]+',
                    'dryad': r'https?://datadryad\.org/[\w/]+',
                    'osf': r'https?://osf\.io/[\w/]+'
                }
                
                found_links = {}
                for name, pattern in patterns.items():
                    matches = re.findall(pattern, content)
                    if matches:
                        found_links[name] = matches
                
                return found_links
        except:
            pass
        
        return {}
    
    def filter_papers_with_quantum_data(self, df):
        """é‡å­å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€å¯èƒ½æ€§ãŒé«˜ã„è«–æ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        
        # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
        def score_paper(row):
            score = 0
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
            title_lower = row['title'].lower()
            if 'experimental' in title_lower:
                score += 3
            if 'measurement' in title_lower:
                score += 2
            if 'benchmark' in title_lower:
                score += 2
            if 'characterization' in title_lower:
                score += 2
            
            # ãƒ‡ãƒ¼ã‚¿é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ•°
            score += len(row['data_keywords']) * 2
            
            # GitHubãƒªãƒ³ã‚¯ã®å­˜åœ¨
            if 'github_links' in row and row['github_links']:
                score += 5
            
            return score
        
        df['data_score'] = df.apply(score_paper, axis=1)
        
        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        df_sorted = df.sort_values('data_score', ascending=False)
        
        # ä¸Šä½ã®è«–æ–‡ã‚’è¿”ã™
        return df_sorted[df_sorted['data_score'] > 0]

# å®Ÿè¡Œé–¢æ•°
def fetch_arxiv_quantum_experiments():
    """arXivã‹ã‚‰é‡å­å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    
    print("=== arXivé‡å­å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿æ¤œç´¢é–‹å§‹ ===")
    
    fetcher = ArxivQuantumDataFetcher()
    
    # 1. è«–æ–‡ã‚’æ¤œç´¢
    print("\n1. é–¢é€£è«–æ–‡ã‚’æ¤œç´¢ä¸­...")
    papers_df = fetcher.search_quantum_datasets(max_results=200)
    print(f"   â†’ åˆè¨ˆ {len(papers_df)} ä»¶ã®è«–æ–‡ã‚’ç™ºè¦‹")
    
    # 2. ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€å¯èƒ½æ€§ãŒé«˜ã„è«–æ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    print("\n2. å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è«–æ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°...")
    filtered_df = fetcher.filter_papers_with_quantum_data(papers_df)
    print(f"   â†’ {len(filtered_df)} ä»¶ã®æœ‰æœ›ãªè«–æ–‡")
    
    # 3. ä¸Šä½10ä»¶ã®è©³ç´°ã‚’è¡¨ç¤º
    print("\n3. æœ€ã‚‚æœ‰æœ›ãªè«–æ–‡ï¼ˆä¸Šä½10ä»¶ï¼‰:")
    print("-" * 80)
    
    for idx, row in filtered_df.head(10).iterrows():
        print(f"\nã€{idx+1}ã€‘{row['title']}")
        print(f"   arXiv ID: {row['arxiv_id']}")
        print(f"   å…¬é–‹æ—¥: {row['published'][:10]}")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚³ã‚¢: {row['data_score']}")
        print(f"   ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(row['data_keywords'])}")
        
        if 'github_links' in row and row['github_links']:
            print(f"   GitHub: {row['github_links']}")
        
        # è¿½åŠ ã®ãƒ‡ãƒ¼ã‚¿ãƒªãƒ³ã‚¯ã‚’å–å¾—
        extra_links = fetcher.get_supplementary_data_links(row['arxiv_id'])
        if extra_links:
            print(f"   è¿½åŠ ãƒªãƒ³ã‚¯: {extra_links}")
    
    # 4. çµæœã‚’ä¿å­˜
    filtered_df.to_csv('arxiv_quantum_data/promising_papers.csv', index=False)
    
    # 5. å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹è«–æ–‡ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    downloadable_papers = create_downloadable_list(filtered_df)
    
    return filtered_df, downloadable_papers

def create_downloadable_list(df):
    """å®Ÿéš›ã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹è«–æ–‡ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ"""
    
    downloadable = []
    
    for _, row in df.iterrows():
        if 'github_links' in row and row['github_links']:
            for github_link in row['github_links']:
                downloadable.append({
                    'title': row['title'],
                    'arxiv_id': row['arxiv_id'],
                    'data_source': 'github',
                    'url': f"https://{github_link}",
                    'type': 'repository'
                })
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä¿å­˜
    download_df = pd.DataFrame(downloadable)
    download_df.to_csv('arxiv_quantum_data/downloadable_sources.csv', index=False)
    
    print(f"\n4. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: {len(downloadable)} ä»¶")
    
    return download_df

if __name__ == "__main__":
    # å®Ÿè¡Œ
    papers, downloadable = fetch_arxiv_quantum_experiments()
    
    print("\n=== æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— ===")
    print("1. promising_papers.csv ã‚’ç¢ºèª")
    print("2. downloadable_sources.csv ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    print("3. download_quantum_datasets.py ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿å–å¾—")
```

### 2. ç‰¹å®šã®æœ‰æœ›ãªè«–æ–‡ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

```python
# known_quantum_datasets.py
"""
æ—¢çŸ¥ã®é‡å­å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒªã‚¹ãƒˆ
"""

KNOWN_QUANTUM_DATASETS = {
    "Google Quantum Supremacy": {
        "arxiv_id": "1910.11333",
        "title": "Quantum supremacy using a programmable superconducting processor",
        "data_url": "https://datadryad.org/stash/dataset/doi:10.5061/dryad.k6t1rj8",
        "description": "53é‡å­ãƒ“ãƒƒãƒˆSycamoreãƒ—ãƒ­ã‚»ãƒƒã‚µã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿"
    },
    
    "IBM Quantum Volume": {
        "arxiv_id": "2008.08571",
        "title": "Demonstration of quantum volume 64 on a superconducting quantum computing system",
        "github": "https://github.com/Qiskit/qiskit-experiments",
        "description": "é‡å­ãƒœãƒªãƒ¥ãƒ¼ãƒ æ¸¬å®šã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿"
    },
    
    "Bell State Tomography": {
        "arxiv_id": "1801.07904",
        "title": "Experimentally exploring compressed sensing quantum tomography",
        "github": "https://github.com/quantumlib/quantum-datasets",
        "description": "BellçŠ¶æ…‹ãƒˆãƒ¢ã‚°ãƒ©ãƒ•ã‚£ãƒ¼ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿"
    },
    
    "NISQ Benchmarks": {
        "arxiv_id": "2003.01293",
        "title": "Quantum Algorithm Implementations for Beginners",
        "supplementary": "https://github.com/lanl/quantum_algorithms",
        "description": "NISQ ãƒ‡ãƒã‚¤ã‚¹ã§ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…ãƒ‡ãƒ¼ã‚¿"
    },
    
    "Quantum Error Characterization": {
        "arxiv_id": "2106.12627",
        "title": "Characterizing quantum instruments: from non-demolition measurements to quantum error correction",
        "zenodo": "https://zenodo.org/record/5012538",
        "description": "é‡å­ã‚¨ãƒ©ãƒ¼ç‰¹æ€§ã®å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿"
    }
}

def get_direct_download_links():
    """ç›´æ¥ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªãƒªãƒ³ã‚¯ã‚’è¿”ã™"""
    
    direct_links = []
    
    for name, info in KNOWN_QUANTUM_DATASETS.items():
        if 'data_url' in info:
            direct_links.append({
                'name': name,
                'url': info['data_url'],
                'type': 'direct'
            })
        elif 'github' in info:
            direct_links.append({
                'name': name,
                'url': info['github'],
                'type': 'github'
            })
        elif 'zenodo' in info:
            direct_links.append({
                'name': name,
                'url': info['zenodo'],
                'type': 'zenodo'
            })
    
    return direct_links
```

### 3. ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
# download_quantum_datasets.py
"""
ç™ºè¦‹ã—ãŸé‡å­ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å®Ÿéš›ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
"""
import requests
import os
import zipfile
import json
from known_quantum_datasets import KNOWN_QUANTUM_DATASETS, get_direct_download_links

def download_dataset(url, save_path):
    """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    
    print(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {url}")
    
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—
        total_size = int(response.headers.get('content-length', 0))
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        with open(save_path, 'wb') as f:
            downloaded = 0
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    # é€²æ—è¡¨ç¤º
                    if total_size > 0:
                        percent = (downloaded / total_size) * 100
                        print(f"  é€²æ—: {percent:.1f}%", end='\r')
        
        print(f"\n  â†’ ä¿å­˜å®Œäº†: {save_path}")
        return True
        
    except Exception as e:
        print(f"  â†’ ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def download_github_data(github_url, save_dir):
    """GitHubãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    
    # ãƒªãƒã‚¸ãƒˆãƒªåã‚’æŠ½å‡º
    parts = github_url.rstrip('/').split('/')
    owner = parts[-2]
    repo = parts[-1]
    
    # GitHubã®ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL
    zip_url = f"https://github.com/{owner}/{repo}/archive/refs/heads/main.zip"
    
    zip_path = os.path.join(save_dir, f"{repo}.zip")
    
    if download_dataset(zip_url, zip_path):
        # ZIPã‚’è§£å‡
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            extract_dir = os.path.join(save_dir, repo)
            zip_ref.extractall(extract_dir)
            print(f"  â†’ è§£å‡å®Œäº†: {extract_dir}")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        find_data_files(extract_dir)

def find_data_files(directory):
    """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™"""
    
    data_extensions = ['.csv', '.json', '.h5', '.hdf5', '.npz', '.pkl', '.dat']
    found_files = []
    
    for root, dirs, files in os.walk(directory):
        for file in files:
            if any(file.endswith(ext) for ext in data_extensions):
                file_path = os.path.join(root, file)
                file_size = os.path.getsize(file_path) / 1024  # KB
                
                if file_size > 1:  # 1KBä»¥ä¸Šã®ãƒ•ã‚¡ã‚¤ãƒ«
                    found_files.append({
                        'path': file_path,
                        'name': file,
                        'size_kb': file_size
                    })
    
    if found_files:
        print(f"\n  ç™ºè¦‹ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«:")
        for f in found_files[:10]:  # æœ€åˆã®10å€‹ã‚’è¡¨ç¤º
            print(f"    - {f['name']} ({f['size_kb']:.1f} KB)")
    
    return found_files

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("=== é‡å­å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ===\n")
    
    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    base_dir = 'downloaded_quantum_data'
    os.makedirs(base_dir, exist_ok=True)
    
    # 1. æ—¢çŸ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    print("1. æ—¢çŸ¥ã®é‡å­ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
    direct_links = get_direct_download_links()
    
    for link in direct_links:
        print(f"\nã€{link['name']}ã€‘")
        
        if link['type'] == 'github':
            save_dir = os.path.join(base_dir, link['name'].replace(' ', '_'))
            os.makedirs(save_dir, exist_ok=True)
            download_github_data(link['url'], save_dir)
        
        elif link['type'] == 'direct':
            filename = link['url'].split('/')[-1]
            save_path = os.path.join(base_dir, filename)
            download_dataset(link['url'], save_path)
    
    # 2. CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è¿½åŠ ã®ã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if os.path.exists('arxiv_quantum_data/downloadable_sources.csv'):
        print("\n\n2. arXivæ¤œç´¢ã§è¦‹ã¤ã‹ã£ãŸãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹")
        import pandas as pd
        
        sources_df = pd.read_csv('arxiv_quantum_data/downloadable_sources.csv')
        
        for _, row in sources_df.head(5).iterrows():  # æœ€åˆã®5å€‹
            print(f"\nã€{row['title'][:50]}...ã€‘")
            
            if row['data_source'] == 'github':
                save_dir = os.path.join(base_dir, f"arxiv_{row['arxiv_id']}")
                os.makedirs(save_dir, exist_ok=True)
                download_github_data(row['url'], save_dir)
    
    print("\n=== ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº† ===")
    print(f"ãƒ‡ãƒ¼ã‚¿ã¯ {base_dir} ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

if __name__ == "__main__":
    main()
```

## ğŸš€ å®Ÿè¡Œæ‰‹é †

1. **ã¾ãšarXivæ¤œç´¢ã‚’å®Ÿè¡Œ**:
```bash
python fetch_arxiv_quantum_data.py
```

2. **çµæœã‚’ç¢ºèª**:
```bash
# æœ‰æœ›ãªè«–æ–‡ãƒªã‚¹ãƒˆã‚’è¦‹ã‚‹
cat arxiv_quantum_data/promising_papers.csv

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª
cat arxiv_quantum_data/downloadable_sources.csv
```

3. **ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**:
```bash
python download_quantum_datasets.py
```

ã“ã‚Œã§å®Ÿéš›ã®é‡å­å®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ãŒæ‰‹ã«å…¥ã‚Šã¾ã™ï¼