"""
è¤‡ç´ æ¼”ç®—ã‚’æ´»ç”¨ã—ãŸCQTè§£æã®å®Ÿè¡Œ
"""
import numpy as np
import pandas as pd
import os
import sys
import json
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã®è¨­å®š
sys.path.append('/home/type37c/projects/CQT_experiments/src')
sys.path.append('/home/type37c/projects/CQT_experiments/data_collection')

from complex_cqt_operations import ComplexCQTAnalyzer, run_complex_analysis, visualize_complex_analysis
from complex_error_detection import ComplexErrorDetector, compute_complex_correlation, detect_quantum_entanglement
from cqt_tracker_v3 import OptimizedCQTTracker

def load_existing_trajectory_data():
    """æ—¢å­˜ã®è»Œè·¡ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    trajectories = {}
    
    # 1. ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®è»Œè·¡
    bell_data_path = '/home/type37c/projects/CQT_experiments/data_collection/collected_data/bell_states/bell_measurement_data.csv'
    
    if os.path.exists(bell_data_path):
        print("BellçŠ¶æ…‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¤‡ç´ è»Œè·¡ã‚’ç”Ÿæˆ...")
        bell_data = pd.read_csv(bell_data_path)
        
        for idx, row in bell_data.iterrows():
            state = row['state']
            counts_str = row['counts']
            
            # countsã®è§£æ
            import ast
            counts_str = counts_str.replace("np.str_('", "'").replace("np.int64(", "").replace("')", "'").replace(")", "")
            counts = ast.literal_eval(counts_str)
            
            # CQTè»Œè·¡ã®ç”Ÿæˆ
            tracker = OptimizedCQTTracker(system_dim=2)
            
            # æ¸¬å®šçµæœã‚’é †æ¬¡å…¥åŠ›
            for outcome_str, count in counts.items():
                # ãƒ“ãƒƒãƒˆåˆ—ã‹ã‚‰æ¸¬å®šçµæœã‚’ç”Ÿæˆ
                for _ in range(min(count // 10, 100)):  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    outcome = int(outcome_str[0])
                    tracker.add_measurement(outcome)
            
            if tracker.trajectory:
                trajectories[f'bell_{state}'] = tracker.trajectory
    
    # 2. å®Ÿé‡å­ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã®è»Œè·¡
    qv_data_path = '/home/type37c/projects/CQT_experiments/data_collection/downloaded_quantum_data/IBM_Quantum_Volume/qiskit-experiments/qiskit-experiments-main/test/library/quantum_volume'
    
    qv_files = {
        'qv_moderate_100': 'qv_data_moderate_noise_100_trials.json',
        'qv_moderate_300': 'qv_data_moderate_noise_300_trials.json',
        'qv_high_noise': 'qv_data_high_noise.json',
        'qv_standard': 'qv_data_70_trials.json'
    }
    
    for label, filename in qv_files.items():
        filepath = os.path.join(qv_data_path, filename)
        
        if os.path.exists(filepath):
            print(f"{label}ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è¤‡ç´ è»Œè·¡ã‚’ç”Ÿæˆ...")
            with open(filepath, 'r') as f:
                data = json.load(f)
            
            # æœ€åˆã®3è©¦è¡Œã‹ã‚‰è»Œè·¡ã‚’ç”Ÿæˆ
            for trial_idx in range(min(3, len(data))):
                trial = data[trial_idx]
                
                if 'counts' in trial:
                    tracker = OptimizedCQTTracker(system_dim=2)
                    counts = trial['counts']
                    
                    # æ¸¬å®šçµæœã®å‡¦ç†
                    for bitstring, count in counts.items():
                        for _ in range(min(count, 20)):  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                            outcome = int(bitstring[0]) if bitstring else 0
                            tracker.add_measurement(outcome)
                    
                    if tracker.trajectory:
                        trajectories[f'{label}_trial_{trial_idx}'] = tracker.trajectory
    
    return trajectories

def generate_synthetic_w_pattern():
    """Wå­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆæˆè»Œè·¡ã‚’ç”Ÿæˆ"""
    print("Wå­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆæˆè»Œè·¡ã‚’ç”Ÿæˆ...")
    
    # Wå­—ã®åŸºæœ¬å½¢çŠ¶ã‚’è¤‡ç´ æ•°ã§è¡¨ç¾
    t = np.linspace(0, 4*np.pi, 200)
    
    # Wå­—ã®å®Ÿéƒ¨ï¼ˆ3ã¤ã®å±±ï¼‰
    real_part = np.sin(t) * np.sin(3*t/4)
    
    # Wå­—ã®è™šéƒ¨ï¼ˆä¸ç¢ºå®Ÿæ€§ã®å¤‰åŒ–ï¼‰
    imag_part = 0.3 * np.sin(2*t) + 0.2 * np.cos(t/2)
    
    # ãƒã‚¤ã‚ºã‚’è¿½åŠ 
    noise_real = 0.1 * np.random.normal(0, 1, len(t))
    noise_imag = 0.05 * np.random.normal(0, 1, len(t))
    
    w_trajectory = (real_part + noise_real) + 1j * (imag_part + noise_imag)
    
    return w_trajectory

def create_error_test_trajectories():
    """ã‚¨ãƒ©ãƒ¼æ¤œå‡ºãƒ†ã‚¹ãƒˆç”¨ã®è»Œè·¡ã‚’ç”Ÿæˆ"""
    print("ã‚¨ãƒ©ãƒ¼æ¤œå‡ºãƒ†ã‚¹ãƒˆç”¨è»Œè·¡ã‚’ç”Ÿæˆ...")
    
    # 1. ã‚¯ãƒªãƒ¼ãƒ³ãªè»Œè·¡
    clean_trajectory = generate_synthetic_w_pattern()
    
    # 2. ãƒ“ãƒƒãƒˆãƒ•ãƒªãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ã‚’å«ã‚€è»Œè·¡
    bitflip_trajectory = clean_trajectory.copy()
    error_positions = np.random.choice(len(bitflip_trajectory), size=len(bitflip_trajectory)//20, replace=False)
    for pos in error_positions:
        # å®Ÿéƒ¨ã®ç¬¦å·ã‚’åè»¢ï¼ˆãƒ“ãƒƒãƒˆãƒ•ãƒªãƒƒãƒ—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼‰
        bitflip_trajectory[pos] = -bitflip_trajectory[pos].real + 1j * bitflip_trajectory[pos].imag
    
    # 3. ä½ç›¸ãƒã‚¤ã‚ºã‚’å«ã‚€è»Œè·¡
    phase_noise_trajectory = clean_trajectory.copy()
    phase_noise = 0.5 * np.random.normal(0, 1, len(phase_noise_trajectory))
    for i, noise in enumerate(phase_noise):
        magnitude = abs(phase_noise_trajectory[i])
        phase = np.angle(phase_noise_trajectory[i]) + noise
        phase_noise_trajectory[i] = magnitude * np.exp(1j * phase)
    
    # 4. æŒ¯å¹…æ¸›è¡°ã‚’å«ã‚€è»Œè·¡
    amplitude_decay_trajectory = clean_trajectory.copy()
    decay_factor = np.exp(-np.linspace(0, 2, len(amplitude_decay_trajectory)))
    amplitude_decay_trajectory *= decay_factor
    
    return {
        'clean': clean_trajectory,
        'bitflip': bitflip_trajectory,
        'phase_noise': phase_noise_trajectory,
        'amplitude_decay': amplitude_decay_trajectory
    }

def main():
    print("=== è¤‡ç´ æ¼”ç®—ã«ã‚ˆã‚‹CQTè§£æé–‹å§‹ ===")
    print(f"é–‹å§‹æ™‚åˆ»: {datetime.now()}")
    
    # 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    print("\n1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿...")
    trajectories = load_existing_trajectory_data()
    print(f"èª­ã¿è¾¼ã¾ã‚ŒãŸè»Œè·¡æ•°: {len(trajectories)}")
    
    # 2. åˆæˆWå­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆ
    print("\n2. åˆæˆWå­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆ...")
    w_pattern = generate_synthetic_w_pattern()
    trajectories['synthetic_w_pattern'] = w_pattern
    
    # 3. å„è»Œè·¡ã®è¤‡ç´ æ¼”ç®—è§£æ
    print("\n3. è¤‡ç´ æ¼”ç®—è§£æã®å®Ÿè¡Œ...")
    analysis_results = {}
    
    for name, trajectory in trajectories.items():
        if len(trajectory) > 10:  # æœ€å°é•·ãƒã‚§ãƒƒã‚¯
            print(f"\n--- {name} ã®è§£æä¸­ ---")
            try:
                analyzer = run_complex_analysis(trajectory, name=name)
                analysis_results[name] = {
                    'analyzer': analyzer,
                    'w_features': analyzer.analyze_w_pattern(),
                    'fourier': analyzer.fourier_analysis(),
                    'invariants': analyzer.calculate_geometric_invariants()
                }
            except Exception as e:
                print(f"ã‚¨ãƒ©ãƒ¼: {name} ã®è§£æã«å¤±æ•—: {e}")
    
    # 4. ã‚¨ãƒ©ãƒ¼æ¤œå‡ºã®å®Ÿè¡Œ
    print("\n4. ã‚¨ãƒ©ãƒ¼æ¤œå‡ºã®å®Ÿè¡Œ...")
    error_test_trajectories = create_error_test_trajectories()
    
    # ã‚¯ãƒªãƒ¼ãƒ³ãªè»Œè·¡ã‚’å‚ç…§ã¨ã—ã¦ä½¿ç”¨
    reference_trajectory = error_test_trajectories['clean']
    detector = ComplexErrorDetector(reference_trajectory)
    
    error_results = {}
    for test_name, test_trajectory in error_test_trajectories.items():
        if test_name != 'clean':  # ã‚¯ãƒªãƒ¼ãƒ³ãªè»Œè·¡ä»¥å¤–ã‚’ãƒ†ã‚¹ãƒˆ
            print(f"\n--- {test_name} ã®ã‚¨ãƒ©ãƒ¼æ¤œå‡ºä¸­ ---")
            errors = detector.detect_errors(test_trajectory)
            error_results[test_name] = errors
            
            print(f"æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼æ•°: {len(errors)}")
            
            # ã‚¨ãƒ©ãƒ¼æ¤œå‡ºã®å¯è¦–åŒ–
            detector.visualize_error_detection(test_trajectory, errors, 
                                             save_path=f'error_detection_{test_name}.png')
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
            detector.generate_error_report(test_trajectory, errors, 
                                         save_path=f'error_report_{test_name}.txt')
    
    # 5. è¤‡ç´ ç›¸é–¢è§£æ
    print("\n5. è¤‡ç´ ç›¸é–¢è§£æ...")
    if len(trajectories) > 1:
        trajectory_names = list(trajectories.keys())
        for i in range(len(trajectory_names)):
            for j in range(i+1, len(trajectory_names)):
                name1, name2 = trajectory_names[i], trajectory_names[j]
                traj1, traj2 = trajectories[name1], trajectories[name2]
                
                # é•·ã•ã‚’åˆã‚ã›ã‚‹
                min_len = min(len(traj1), len(traj2))
                traj1_trimmed = traj1[:min_len]
                traj2_trimmed = traj2[:min_len]
                
                correlation = compute_complex_correlation(traj1_trimmed, traj2_trimmed)
                print(f"{name1} vs {name2}: è¤‡ç´ ç›¸é–¢ = {correlation:.4f}")
                
                # é‡å­ã‚‚ã¤ã‚Œæ¤œå‡º
                entanglement = detect_quantum_entanglement(traj1_trimmed, traj2_trimmed)
                if entanglement['entangled']:
                    print(f"  â†’ é‡å­ã‚‚ã¤ã‚Œæ¤œå‡º! ã‚¹ã‚³ã‚¢: {entanglement['score']:.4f}")
    
    # 6. æ¯”è¼ƒåˆ†æã¨ã‚µãƒãƒªãƒ¼
    print("\n6. æ¯”è¼ƒåˆ†æã¨ã‚µãƒãƒªãƒ¼...")
    
    # ç‰¹å¾´é‡ã®æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«
    feature_comparison = []
    for name, result in analysis_results.items():
        w_features = result['w_features']
        fourier = result['fourier']
        invariants = result['invariants']
        
        feature_comparison.append({
            'name': name,
            'winding_number': w_features['winding_number'],
            'fractal_dimension': w_features['fractal_dimension'],
            'spectral_entropy': fourier['spectral_entropy'],
            'total_length': invariants['total_length'],
            'enclosed_area': invariants['enclosed_area'],
            'asymmetry': invariants['asymmetry']
        })
    
    comparison_df = pd.DataFrame(feature_comparison)
    print("\n=== ç‰¹å¾´é‡æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ« ===")
    print(comparison_df.round(4))
    
    # çµæœã‚’CSVã§ä¿å­˜
    comparison_df.to_csv('complex_cqt_feature_comparison.csv', index=False)
    print("\nç‰¹å¾´é‡æ¯”è¼ƒãƒ†ãƒ¼ãƒ–ãƒ«ã‚’CSVã§ä¿å­˜ã—ã¾ã—ãŸ: complex_cqt_feature_comparison.csv")
    
    # 7. æœ€çµ‚ã‚µãƒãƒªãƒ¼
    print(f"\n=== è¤‡ç´ æ¼”ç®—CQTè§£æå®Œäº† ===")
    print(f"çµ‚äº†æ™‚åˆ»: {datetime.now()}")
    print(f"è§£æã•ã‚ŒãŸè»Œè·¡æ•°: {len(analysis_results)}")
    print(f"ã‚¨ãƒ©ãƒ¼æ¤œå‡ºãƒ†ã‚¹ãƒˆæ•°: {len(error_results)}")
    print(f"ç”Ÿæˆã•ã‚ŒãŸå¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(analysis_results) + len(error_results)}")
    
    # ä¸»è¦ãªç™ºè¦‹
    print("\n=== ä¸»è¦ãªç™ºè¦‹ ===")
    if analysis_results:
        # æœ€ã‚‚è¤‡é›‘ãªè»Œè·¡
        max_entropy = max(analysis_results.items(), 
                         key=lambda x: x[1]['fourier']['spectral_entropy'])
        print(f"æœ€ã‚‚è¤‡é›‘ãªè»Œè·¡: {max_entropy[0]} (ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {max_entropy[1]['fourier']['spectral_entropy']:.3f})")
        
        # æœ€ã‚‚è¦å‰‡çš„ãªè»Œè·¡
        min_entropy = min(analysis_results.items(), 
                         key=lambda x: x[1]['fourier']['spectral_entropy'])
        print(f"æœ€ã‚‚è¦å‰‡çš„ãªè»Œè·¡: {min_entropy[0]} (ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼: {min_entropy[1]['fourier']['spectral_entropy']:.3f})")
        
        # ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®ç¯„å›²
        fractal_dims = [result['w_features']['fractal_dimension'] 
                       for result in analysis_results.values() 
                       if result['w_features']['fractal_dimension'] is not None]
        if fractal_dims:
            print(f"ãƒ•ãƒ©ã‚¯ã‚¿ãƒ«æ¬¡å…ƒã®ç¯„å›²: {min(fractal_dims):.3f} - {max(fractal_dims):.3f}")
    
    print("\nğŸ¯ è¤‡ç´ æ¼”ç®—ã«ã‚ˆã‚‹CQTç†è«–ã®æ·±åŒ–è§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main()